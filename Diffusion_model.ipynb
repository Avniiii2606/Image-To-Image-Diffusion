{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNBPxIiOHVxo5Iv+rONYKbk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Avniiii2606/Image-To-Image-Diffusion/blob/main/Diffusion_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "\n",
        "path = kagglehub.dataset_download(\"ashwingupta3012/human-faces\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "path1 = kagglehub.dataset_download(\"elibooklover/victorian400\")\n",
        "\n",
        "print(\"Path to dataset files:\", path1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0jRijSdBHjK",
        "outputId": "8bbec2f7-c0d0-4b8a-bf06-3136ae034211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/ashwingupta3012/human-faces?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.82G/1.82G [00:22<00:00, 85.0MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/ashwingupta3012/human-faces/versions/1\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/elibooklover/victorian400?dataset_version_number=5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 484M/484M [00:04<00:00, 121MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/elibooklover/victorian400/versions/5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMkYm5SGx7uG",
        "outputId": "2f506901-3099-4ec9-df6a-08ff1f19ae65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "lKrOqCJz1MZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "XIKqPQDXB0h9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import wandb  # for experiment tracking\n",
        "from pathlib import Path\n",
        "import math\n",
        "\n",
        "class FaceTranslationDataset(Dataset):\n",
        "    \"\"\"Custom dataset for paired face images (painting/statue -> real)\"\"\"\n",
        "    def __init__(self, source_dir, target_dir, image_size=256):\n",
        "        # Use Path.rglob to search for images recursively in subfolders\n",
        "        self.source_paths = sorted(Path(source_dir).rglob('*.jpg'))\n",
        "        self.target_paths = sorted(Path(target_dir).rglob('*.jpg'))\n",
        "\n",
        "        # Print the number of images found\n",
        "        print(f\"Found {len(self.source_paths)} source images\")\n",
        "        print(f\"Found {len(self.target_paths)} target images\")\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.CenterCrop(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        # Ensure both source and target have the same number of images\n",
        "        return min(len(self.source_paths), len(self.target_paths))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        source_img = Image.open(self.source_paths[idx]).convert('RGB')\n",
        "        target_img = Image.open(self.target_paths[idx]).convert('RGB')\n",
        "\n",
        "        return {\n",
        "            'source': self.transform(source_img),\n",
        "            'target': self.transform(target_img)\n",
        "        }\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    \"\"\"Fixed cross attention module for conditioning\"\"\"\n",
        "    def __init__(self, channels, num_heads=8):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.channels = channels\n",
        "        self.scale = (channels // num_heads) ** -0.5\n",
        "\n",
        "        # Fix: Initialize GroupNorm with the correct number of channels\n",
        "        self.norm_x = nn.GroupNorm(1, channels)  # Use separate GroupNorms\n",
        "        self.norm_context = nn.GroupNorm(1, channels)  # For the context with 'channels' channels\n",
        "\n",
        "        self.to_q = nn.Conv2d(channels, channels, 1)\n",
        "        self.to_k = nn.Conv2d(channels, channels, 1)  # Change input channels to 'channels'\n",
        "        self.to_v = nn.Conv2d(channels, channels, 1)  # Change input channels to 'channels'\n",
        "        self.to_out = nn.Conv2d(channels, channels, 1)  # Initialize the context norm with the number of channels in the input tensor x\n",
        "\n",
        "        # Define context_proj here\n",
        "        self.context_proj = nn.Conv2d(in_channels=3, out_channels=channels, kernel_size=1) # Assuming context has 3 channels\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        batch, c, h, w = x.shape\n",
        "\n",
        "        # Project the context to the desired number of channels before normalization\n",
        "        context = self.context_proj(context)\n",
        "\n",
        "\n",
        "        # Normalize inputs using separate GroupNorms\n",
        "        x = self.norm_x(x)\n",
        "        context = self.norm_context(context)\n",
        "\n",
        "        # Create Q, K, V\n",
        "        q = self.to_q(x)\n",
        "        k = self.to_k(context)\n",
        "        v = self.to_v(context)\n",
        "\n",
        "        # Reshape for attention\n",
        "        q = q.view(batch, self.num_heads, c // self.num_heads, h * w)\n",
        "        k = k.view(batch, self.num_heads, c // self.num_heads, h * w)\n",
        "        v = v.view(batch, self.num_heads, c // self.num_heads, h * w)\n",
        "\n",
        "        # Transpose for attention computation\n",
        "        q, k, v = map(lambda t: t.transpose(-2, -1), (q, k, v))\n",
        "\n",
        "        # Attention\n",
        "        attention = torch.softmax(torch.matmul(q, k.transpose(-2, -1)) * self.scale, dim=-1)\n",
        "        out = torch.matmul(attention, v)\n",
        "\n",
        "        # Reshape back\n",
        "        out = out.transpose(-2, -1).reshape(batch, c, h, w)\n",
        "        return self.to_out(out) + x\n",
        "\n",
        "class ConditionalUNet(nn.Module):\n",
        "    \"\"\"Updated U-Net with fixed cross-attention\"\"\"\n",
        "    def __init__(self, in_channels=3, out_channels=3, time_emb_dim=256):\n",
        "        super().__init__()\n",
        "\n",
        "        # Time embedding\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
        "            nn.Linear(time_emb_dim, time_emb_dim * 2),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(time_emb_dim * 2, time_emb_dim)\n",
        "        )\n",
        "\n",
        "        # Initial convolution\n",
        "        self.conv_in = nn.Conv2d(in_channels, 64, 3, padding=1)\n",
        "\n",
        "        # Encoder\n",
        "        self.down1 = nn.ModuleList([\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            CrossAttention(128),\n",
        "            nn.Conv2d(128, 128, 3, padding=1)\n",
        "        ])\n",
        "\n",
        "        self.down2 = nn.ModuleList([\n",
        "            nn.Conv2d(128, 256, 3, padding=1),\n",
        "            CrossAttention(256),\n",
        "            nn.Conv2d(256, 256, 3, padding=1)\n",
        "        ])\n",
        "\n",
        "        self.down3 = nn.ModuleList([\n",
        "            nn.Conv2d(256, 512, 3, padding=1),\n",
        "            CrossAttention(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1)\n",
        "        ])\n",
        "\n",
        "        # Middle\n",
        "        self.mid = nn.ModuleList([\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            CrossAttention(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1)\n",
        "        ])\n",
        "\n",
        "        # Decoder\n",
        "        self.up1 = nn.ModuleList([\n",
        "            nn.ConvTranspose2d(1024, 256, 2, stride=2),\n",
        "            CrossAttention(256),\n",
        "            nn.Conv2d(256, 256, 3, padding=1)\n",
        "        ])\n",
        "\n",
        "        self.up2 = nn.ModuleList([\n",
        "            nn.ConvTranspose2d(512, 128, 2, stride=2),\n",
        "            CrossAttention(128),\n",
        "            nn.Conv2d(128, 128, 3, padding=1)\n",
        "        ])\n",
        "\n",
        "        self.up3 = nn.ModuleList([\n",
        "            nn.ConvTranspose2d(256, 64, 2, stride=2),\n",
        "            CrossAttention(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1)\n",
        "        ])\n",
        "\n",
        "        # Final convolution\n",
        "        self.conv_out = nn.Conv2d(64, out_channels, 1)\n",
        "\n",
        "        # Downsample and upsample operations\n",
        "        self.downs = nn.ModuleList([\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.MaxPool2d(2)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, t, context):\n",
        "        # Time embedding\n",
        "        t = self.time_mlp(t)[:, :, None, None]  # Add spatial dimensions\n",
        "\n",
        "        # Initial conv\n",
        "        x = self.conv_in(x)\n",
        "\n",
        "        # Cache residuals\n",
        "        residuals = []\n",
        "\n",
        "        # Encoder\n",
        "        for i, down in enumerate([self.down1, self.down2, self.down3]):\n",
        "            residuals.append(x)\n",
        "            x = F.gelu(down[0](x))\n",
        "            x = down[1](x, context)  # Cross attention\n",
        "            x = F.gelu(down[2](x))\n",
        "            x = self.downs[i](x)\n",
        "\n",
        "        # Middle\n",
        "        x = F.gelu(self.mid[0](x))\n",
        "        x = self.mid[1](x, context)  # Cross attention\n",
        "        x = F.gelu(self.mid[2](x))\n",
        "\n",
        "        # Decoder\n",
        "        for i, up in enumerate([self.up1, self.up2, self.up3]):\n",
        "            x = up[0](torch.cat([x, residuals[-i-1]], dim=1))\n",
        "            x = up[1](x, context)  # Cross attention\n",
        "            x = F.gelu(up[2](x))\n",
        "\n",
        "        return self.conv_out(x)\n",
        "\n",
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    \"\"\"Time embedding module\"\"\"\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings\n",
        "\n",
        "def train(source_dir, target_dir, num_epochs=100, batch_size=4, device=\"cuda\", gradient_accumulation_steps=1): # add gradient_accumulation_steps\n",
        "    # Initialize dataset and dataloader\n",
        "    dataset = FaceTranslationDataset(source_dir, target_dir, image_size=128) # Reduce image size to 128x128\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "    # Initialize model and diffusion\n",
        "    model = ConditionalUNet().to(device)\n",
        "    diffusion = FaceTranslationDiffusion()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
        "            source_images = batch['source'].to(device)\n",
        "            target_images = batch['target'].to(device)\n",
        "\n",
        "            # Sample random timesteps\n",
        "            t = torch.randint(0, diffusion.noise_steps, (source_images.shape[0],)).to(device)\n",
        "\n",
        "            # Add noise to target images\n",
        "            noisy_target, noise = diffusion.noise_images(target_images, t)\n",
        "\n",
        "            # Predict noise\n",
        "            predicted_noise = model(noisy_target, t, source_images)\n",
        "\n",
        "             # Calculate loss\n",
        "            loss = F.mse_loss(noise, predicted_noise)\n",
        "\n",
        "            # Gradient Accumulation\n",
        "            loss = loss / gradient_accumulation_steps # scale loss\n",
        "            loss.backward()\n",
        "\n",
        "            if (batch_idx + 1) % gradient_accumulation_steps == 0: # update every n steps\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            # Optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Print progress\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Save sample generations periodically\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            save_sample_generations(model, diffusion, dataloader, epoch, device)\n",
        "\n",
        "def save_sample_generations(model, diffusion, dataloader, epoch, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        sample_source = next(iter(dataloader))['source'][:1].to(device)\n",
        "        sample_result = diffusion.sample(model, sample_source)\n",
        "\n",
        "        # Save the result\n",
        "        save_image(\n",
        "            torch.cat([sample_source, sample_result], dim=0),\n",
        "            f\"samples/epoch_{epoch+1}.png\",\n",
        "            normalize=True,\n",
        "            nrow=2\n",
        "        )\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation, PillowWriter\n",
        "import imageio\n",
        "from IPython.display import display, HTML\n",
        "import numpy as np\n",
        "from torchvision.utils import make_grid\n",
        "import io\n",
        "\n",
        "class FaceTranslationVisualizer:\n",
        "    def __init__(self, model, diffusion, device=\"cuda\"):\n",
        "        self.model = model\n",
        "        self.diffusion = diffusion\n",
        "        self.device = device\n",
        "\n",
        "    def tensor_to_image(self, tensor):\n",
        "        \"\"\"Convert tensor to numpy image\"\"\"\n",
        "        img = tensor.cpu().detach()\n",
        "        img = (img.clamp(-1, 1) + 1) / 2\n",
        "        img = img.permute(1, 2, 0).numpy()\n",
        "        return (img * 255).astype(np.uint8)\n",
        "\n",
        "    def create_comparison_grid(self, source_img, generated_img):\n",
        "        \"\"\"Create a grid with source and generated images\"\"\"\n",
        "        # Ensure inputs are in the right format\n",
        "        if isinstance(source_img, np.ndarray):\n",
        "            source_img = torch.from_numpy(source_img).permute(2, 0, 1)\n",
        "        if isinstance(generated_img, np.ndarray):\n",
        "            generated_img = torch.from_numpy(generated_img).permute(2, 0, 1)\n",
        "\n",
        "        # Create grid\n",
        "        grid = make_grid(\n",
        "            [source_img, generated_img],\n",
        "            nrow=2,\n",
        "            normalize=True,\n",
        "            value_range=(-1, 1)\n",
        "        )\n",
        "        return grid\n",
        "\n",
        "    def save_generation_process(self, source_image, save_path=\"generation_process.gif\",\n",
        "                              num_frames=50, fps=10):\n",
        "        \"\"\"Generate and save the transformation process as a GIF\"\"\"\n",
        "        self.model.eval()\n",
        "        frames = []\n",
        "\n",
        "        # Prepare source image\n",
        "        if not isinstance(source_image, torch.Tensor):\n",
        "            transform = torchvision.transforms.ToTensor()\n",
        "            source_image = transform(source_image).unsqueeze(0).to(self.device)\n",
        "            source_image = source_image * 2 - 1  # Scale to [-1, 1]\n",
        "\n",
        "        # Initialize with random noise\n",
        "        x = torch.randn((1, 3, self.diffusion.img_size, self.diffusion.img_size)).to(self.device)\n",
        "\n",
        "        step_size = self.diffusion.noise_steps // num_frames\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in reversed(range(0, self.diffusion.noise_steps, step_size)):\n",
        "                # Get current timestep\n",
        "                t = torch.tensor([i]).to(self.device)\n",
        "\n",
        "                # Predict noise and denoise\n",
        "                predicted_noise = self.model(x, t, source_image)\n",
        "\n",
        "                alpha = self.diffusion.alphas[i]\n",
        "                alpha_hat = self.diffusion.alpha_hat[i]\n",
        "                beta = self.diffusion.betas[i]\n",
        "\n",
        "                if i > 0:\n",
        "                    noise = torch.randn_like(x)\n",
        "                else:\n",
        "                    noise = torch.zeros_like(x)\n",
        "\n",
        "                x = 1 / torch.sqrt(alpha) * (\n",
        "                    x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise\n",
        "                ) + torch.sqrt(beta) * noise\n",
        "\n",
        "                # Create comparison frame\n",
        "                if i % step_size == 0:\n",
        "                    current_img = (x.clamp(-1, 1) + 1) / 2\n",
        "                    grid = self.create_comparison_grid(\n",
        "                        source_image[0],\n",
        "                        current_img[0]\n",
        "                    )\n",
        "                    grid_img = (grid.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
        "                    frames.append(Image.fromarray(grid_img))\n",
        "\n",
        "        # Save as GIF\n",
        "        frames[0].save(\n",
        "            save_path,\n",
        "            save_all=True,\n",
        "            append_images=frames[1:],\n",
        "            duration=1000/fps,\n",
        "            loop=0\n",
        "        )\n",
        "        return frames\n",
        "\n",
        "    def display_interactive(self, source_image, figsize=(12, 6)):\n",
        "        \"\"\"Display interactive visualization in notebook\"\"\"\n",
        "        frames = self.save_generation_process(source_image)\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=figsize)\n",
        "        plt.axis('off')\n",
        "\n",
        "        def animate(frame):\n",
        "            ax.clear()\n",
        "            ax.imshow(frame)\n",
        "            ax.axis('off')\n",
        "            ax.set_title('Left: Source Image | Right: Generated Image')\n",
        "\n",
        "        anim = FuncAnimation(\n",
        "            fig,\n",
        "            animate,\n",
        "            frames=frames,\n",
        "            interval=100,\n",
        "            repeat=True\n",
        "        )\n",
        "        plt.close()\n",
        "        return anim\n",
        "\n",
        "    def display_static_comparison(self, source_image, generated_image, figsize=(12, 6)):\n",
        "        \"\"\"Display static comparison of source and generated images\"\"\"\n",
        "        plt.figure(figsize=figsize)\n",
        "\n",
        "        grid = self.create_comparison_grid(source_image, generated_image)\n",
        "        grid_img = grid.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "        plt.imshow(grid_img)\n",
        "        plt.axis('off')\n",
        "        plt.title('Left: Source Image | Right: Generated Image')\n",
        "        plt.show()\n",
        "\n",
        "# Modify the training loop to include visualization\n",
        "def train_with_visualization(source_dir, target_dir, num_epochs=100, batch_size=8,\n",
        "                           device=\"cuda\", save_dir=\"results\"):\n",
        "    # Initialize models and dataset as before\n",
        "    model = ConditionalUNet().to(device)\n",
        "    diffusion = FaceTranslationDiffusion()\n",
        "    visualizer = FaceTranslationVisualizer(model, diffusion, device)\n",
        "\n",
        "    # Create save directory\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Training loop with visualization\n",
        "    for epoch in range(num_epochs):\n",
        "        # ... (previous training code) ...\n",
        "\n",
        "        # Generate and save visualization every n epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                # Get a sample image\n",
        "                sample_source = next(iter(dataloader))['source'][:1].to(device)\n",
        "\n",
        "                # Generate and save the transformation process\n",
        "                save_path = os.path.join(save_dir, f\"epoch_{epoch+1}_process.gif\")\n",
        "                visualizer.save_generation_process(\n",
        "                    sample_source,\n",
        "                    save_path=save_path\n",
        "                )\n",
        "\n",
        "                # Generate final image and save comparison\n",
        "                final_generated = diffusion.sample(model, sample_source)\n",
        "                plt.figure(figsize=(12, 6))\n",
        "                visualizer.display_static_comparison(\n",
        "                    sample_source[0],\n",
        "                    final_generated[0]\n",
        "                )\n",
        "                plt.savefig(os.path.join(save_dir, f\"epoch_{epoch+1}_comparison.png\"))\n",
        "                plt.close()\n",
        "\n",
        "# Example usage\n",
        "def demo_visualization(source_image_path):\n",
        "    # Load and preprocess source image\n",
        "    source_image = Image.open(source_image_path).convert('RGB')\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(256),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    source_tensor = transform(source_image).unsqueeze(0)\n",
        "\n",
        "    # Initialize models\n",
        "    model = ConditionalUNet().to(\"cuda\")\n",
        "    diffusion = FaceTranslationDiffusion()\n",
        "    visualizer = FaceTranslationVisualizer(model, diffusion)\n",
        "\n",
        "    # Load model weights (assuming you have trained the model)\n",
        "    model.load_state_dict(torch.load(\"model_weights.pth\"))\n",
        "\n",
        "    # Generate and display transformation process\n",
        "    print(\"Generating transformation process...\")\n",
        "    anim = visualizer.display_interactive(source_tensor)\n",
        "\n",
        "    # Generate final image and display comparison\n",
        "    print(\"Generating final comparison...\")\n",
        "    with torch.no_grad():\n",
        "        generated_image = diffusion.sample(model, source_tensor.to(\"cuda\"))\n",
        "        visualizer.display_static_comparison(source_tensor[0], generated_image[0])\n",
        "\n",
        "    return anim\n",
        "\n",
        "\n",
        "import torch\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FaceTranslationDiffusion:\n",
        "    def __init__(self, noise_steps=1000, beta_start=1e-4, beta_end=0.02, img_size=256, device=\"cuda\"):\n",
        "        self.noise_steps = noise_steps\n",
        "        self.beta_start = beta_start\n",
        "        self.beta_end = beta_end\n",
        "        self.img_size = img_size\n",
        "        self.device = device\n",
        "\n",
        "        # Define beta schedule\n",
        "        self.betas = torch.linspace(beta_start, beta_end, noise_steps, device=self.device)  # betas on device\n",
        "        self.alphas = 1. - self.betas\n",
        "        self.alpha_hat = torch.cumprod(self.alphas, dim=0)  # alpha_hat on device\n",
        "\n",
        "    def noise_images(self, x, t):\n",
        "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n",
        "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]\n",
        "        ε = torch.randn_like(x)\n",
        "        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * ε, ε\n",
        "\n",
        "    def sample_timesteps(self, n):\n",
        "        return torch.randint(0, self.noise_steps, (n,), device=self.device)\n",
        "\n",
        "    def sample(self, model, source_image, n=1):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            x = torch.randn((n, 3, self.img_size, self.img_size), device=self.device)  # x on device\n",
        "            for i in tqdm(reversed(range(0, self.noise_steps)), desc='sampling loop time step', total=self.noise_steps):\n",
        "                t = torch.full((n,), i, device=self.device, dtype=torch.long)  # t on device\n",
        "                predicted_noise = model(x, t, source_image)\n",
        "                alpha = self.alphas[t][:, None, None, None]\n",
        "                alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
        "                beta = self.betas[t][:, None, None, None]\n",
        "                if i > 0:\n",
        "                    noise = torch.randn_like(x)\n",
        "                else:\n",
        "                    noise = torch.zeros_like(x)\n",
        "                x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
        "        # Return the generated image in the range [-1, 1] for consistency\n",
        "        return x.clamp(-1, 1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage\n",
        "    source_dir = \"/root/.cache/kagglehub/datasets/elibooklover/victorian400/versions/5\"\n",
        "    target_dir = \"/root/.cache/kagglehub/datasets/ashwingupta3012/human-faces/versions/1\"\n",
        "\n",
        "    train(\n",
        "        source_dir=source_dir,\n",
        "        target_dir=target_dir,\n",
        "        num_epochs=100,\n",
        "        batch_size=2\n",
        "    )\n",
        "\n",
        "    source_image_path = \"/content/drive/MyDrive/victorian_potrait.jpg.png\"\n",
        "    animation = demo_visualization(source_image_path)\n",
        "\n",
        "    # Save animation\n",
        "    animation.save('transformation.gif', writer='pillow')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "5xao_CoEuqAV",
        "outputId": "2ef7a0cb-203d-4ca7-844e-956cc2ba6cc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8 source images\n",
            "Found 6973 target images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 1/100:   0%|          | 0/4 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 14.39 GiB is free. Process 17453 has 354.00 MiB memory in use. Of the allocated memory 202.12 MiB is allocated by PyTorch, and 23.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-fa4f0348a848>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0mtarget_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/root/.cache/kagglehub/datasets/ashwingupta3012/human-faces/versions/1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m     train(\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[0msource_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mtarget_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-fa4f0348a848>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(source_dir, target_dir, num_epochs, batch_size, device, gradient_accumulation_steps)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;31m# Predict noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0mpredicted_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoisy_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m              \u001b[0;31m# Calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-fa4f0348a848>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, t, context)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mresiduals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdown\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdown\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Cross attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdown\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdowns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-fa4f0348a848>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 14.39 GiB is free. Process 17453 has 354.00 MiB memory in use. Of the allocated memory 202.12 MiB is allocated by PyTorch, and 23.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class DiffusionModel:\n",
        "    def __init__(self, noise_steps=1000, beta_start=1e-4, beta_end=0.02, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        \"\"\"\n",
        "        Initialize diffusion model parameters\n",
        "\n",
        "        Args:\n",
        "            noise_steps (int): Number of diffusion steps\n",
        "            beta_start (float): Starting noise scale\n",
        "            beta_end (float): Ending noise scale\n",
        "            device (str): Device to store tensors on ('cuda' or 'cpu')\n",
        "        \"\"\"\n",
        "        self.noise_steps = noise_steps\n",
        "        self.device = device\n",
        "\n",
        "        # Generate noise schedule and move to device\n",
        "        self.betas = torch.linspace(beta_start, beta_end, noise_steps).to(device)\n",
        "        self.alphas = 1 - self.betas\n",
        "        self.alpha_prod = torch.cumprod(self.alphas, dim=0).to(device)\n",
        "        self.sqrt_alpha_prod = torch.sqrt(self.alpha_prod).to(device)\n",
        "        self.sqrt_one_minus_alpha_prod = torch.sqrt(1 - self.alpha_prod).to(device)\n",
        "\n",
        "    def forward_diffusion(self, x0, t):\n",
        "        \"\"\"\n",
        "        Add noise to input image at timestep t\n",
        "\n",
        "        Args:\n",
        "            x0 (torch.Tensor): Original image\n",
        "            t (torch.Tensor): Timestep\n",
        "\n",
        "        Returns:\n",
        "            Noisy image and noise\n",
        "        \"\"\"\n",
        "        noise = torch.randn_like(x0)\n",
        "        sqrt_alpha_prod_t = self.sqrt_alpha_prod[t].view(-1, 1, 1, 1)\n",
        "        sqrt_one_minus_alpha_prod_t = self.sqrt_one_minus_alpha_prod[t].view(-1, 1, 1, 1)\n",
        "\n",
        "        noisy_x = sqrt_alpha_prod_t * x0 + sqrt_one_minus_alpha_prod_t * noise\n",
        "        return noisy_x, noise\n",
        "\n",
        "    class UNet(nn.Module):\n",
        "        def __init__(self, in_channels=3, out_channels=3, base_channels=64):\n",
        "            \"\"\"\n",
        "            U-Net architecture for noise prediction\n",
        "\n",
        "            Args:\n",
        "                in_channels (int): Input image channels\n",
        "                out_channels (int): Output noise channels\n",
        "                base_channels (int): Base convolution channels\n",
        "            \"\"\"\n",
        "            super().__init__()\n",
        "\n",
        "            # Encoder\n",
        "            self.enc1 = self._block(in_channels, base_channels)\n",
        "            self.enc2 = self._block(base_channels, base_channels * 2)\n",
        "            self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "            # Middle\n",
        "            self.middle = self._block(base_channels * 2, base_channels * 4)\n",
        "\n",
        "            # Decoder\n",
        "            self.upconv2 = nn.ConvTranspose2d(base_channels * 4, base_channels * 2, kernel_size=2, stride=2)\n",
        "            self.dec2 = self._block(base_channels * 4, base_channels * 2)\n",
        "\n",
        "            self.upconv1 = nn.ConvTranspose2d(base_channels * 2, base_channels, kernel_size=2, stride=2)\n",
        "            self.dec1 = self._block(base_channels * 2, base_channels)\n",
        "\n",
        "            self.final_conv = nn.Conv2d(base_channels, out_channels, kernel_size=1)\n",
        "\n",
        "        def _block(self, in_channels, out_channels):\n",
        "            \"\"\"Standard convolutional block with normalization and activation\"\"\"\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "\n",
        "        def forward(self, x, t):\n",
        "            \"\"\"\n",
        "            Forward pass through U-Net\n",
        "\n",
        "            Args:\n",
        "                x (torch.Tensor): Noisy input image\n",
        "                t (torch.Tensor): Timestep\n",
        "\n",
        "            Returns:\n",
        "                Predicted noise\n",
        "            \"\"\"\n",
        "            # Time embedding (simple positional encoding)\n",
        "            t_emb = torch.sin(t.float())\n",
        "\n",
        "            # Encoder\n",
        "            enc1 = self.enc1(x)\n",
        "            enc2 = self.enc2(self.pool(enc1))\n",
        "\n",
        "            # Middle\n",
        "            middle = self.middle(self.pool(enc2))\n",
        "\n",
        "            # Decoder\n",
        "            dec2 = self.upconv2(middle)\n",
        "            dec2 = torch.cat([dec2, enc2], dim=1)\n",
        "            dec2 = self.dec2(dec2)\n",
        "\n",
        "            dec1 = self.upconv1(dec2)\n",
        "            dec1 = torch.cat([dec1, enc1], dim=1)\n",
        "            dec1 = self.dec1(dec1)\n",
        "\n",
        "            return self.final_conv(dec1)\n",
        "\n",
        "    def reverse_diffusion(self, x, model):\n",
        "        \"\"\"\n",
        "        Reverse the diffusion process to generate an image\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Starting noisy image\n",
        "            model (nn.Module): Noise prediction model\n",
        "\n",
        "        Returns:\n",
        "            Denoised image\n",
        "        \"\"\"\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for t in reversed(range(self.noise_steps)):\n",
        "                t_tensor = torch.full((x.shape[0],), t, dtype=torch.long)\n",
        "                predicted_noise = model(x, t_tensor)\n",
        "\n",
        "                # Compute coefficients\n",
        "                beta_t = self.betas[t]\n",
        "                one_minus_alpha_t = 1 - self.alphas[t]\n",
        "                sqrt_one_minus_alpha_prod_t = self.sqrt_one_minus_alpha_prod[t]\n",
        "\n",
        "                # Compute noise scaling\n",
        "                noise_scaling = beta_t / sqrt_one_minus_alpha_prod_t\n",
        "\n",
        "                # Denoise\n",
        "                x = (1 / torch.sqrt(self.alphas[t])) * (\n",
        "                    x - noise_scaling * predicted_noise\n",
        "                )\n",
        "\n",
        "                # Add noise if not at the final step\n",
        "                if t > 0:\n",
        "                    noise = torch.randn_like(x)\n",
        "                    x += torch.sqrt(self.betas[t]) * noise\n",
        "\n",
        "        return x.clamp(0, 1)\n",
        "\n",
        "class DiffusionTrainer:\n",
        "    def __init__(self, data_dir, image_size=64, batch_size=32):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.image_size = image_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Initialize dataset and dataloader\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.CenterCrop(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "\n",
        "        self.dataset = datasets.ImageFolder(\n",
        "            root=data_dir,\n",
        "            transform=self.transform\n",
        "        )\n",
        "\n",
        "        self.dataloader = DataLoader(\n",
        "            self.dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=4,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        # Initialize model and diffusion process\n",
        "        self.diffusion = DiffusionModel()\n",
        "        self.model = self.diffusion.UNet(in_channels=3, out_channels=3).to(self.device)\n",
        "\n",
        "        # Initialize optimizer\n",
        "        self.optimizer = Adam(self.model.parameters(), lr=2e-4)\n",
        "\n",
        "    def train(self, num_epochs=100, save_interval=10):\n",
        "        \"\"\"Train the diffusion model\"\"\"\n",
        "        for epoch in range(num_epochs):\n",
        "            self.model.train()\n",
        "            total_loss = 0\n",
        "\n",
        "            progress_bar = tqdm(self.dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "            for batch_idx, (images, _) in enumerate(progress_bar):\n",
        "                images = images.to(self.device)\n",
        "                batch_size = images.shape[0]\n",
        "\n",
        "                # Sample random timesteps\n",
        "                t = torch.randint(0, self.diffusion.noise_steps, (batch_size,),\n",
        "                                device=self.device)\n",
        "\n",
        "                # Forward diffusion\n",
        "                noisy_images, noise = self.diffusion.forward_diffusion(images, t)\n",
        "\n",
        "                # Predict noise\n",
        "                predicted_noise = self.model(noisy_images, t)\n",
        "\n",
        "                # Calculate loss\n",
        "                loss = nn.MSELoss()(predicted_noise, noise)\n",
        "\n",
        "                # Backpropagation\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                # Update progress bar\n",
        "                progress_bar.set_postfix({\"Loss\": total_loss / (batch_idx + 1)})\n",
        "\n",
        "            # Save checkpoint\n",
        "            if (epoch + 1) % save_interval == 0:\n",
        "                self.save_checkpoint(epoch + 1)\n",
        "                self.generate_samples(epoch + 1)\n",
        "\n",
        "    def save_checkpoint(self, epoch):\n",
        "        \"\"\"Save model checkpoint\"\"\"\n",
        "        checkpoint_dir = \"checkpoints\"\n",
        "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f\"diffusion_epoch_{epoch}.pt\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "        }, checkpoint_path)\n",
        "\n",
        "    def generate_samples(self, epoch, num_samples=16):\n",
        "        \"\"\"Generate and save sample images\"\"\"\n",
        "        self.model.eval()\n",
        "        samples_dir = \"samples\"\n",
        "        os.makedirs(samples_dir, exist_ok=True)\n",
        "\n",
        "        # Start from random noise\n",
        "        x = torch.randn(num_samples, 3, self.image_size, self.image_size).to(self.device)\n",
        "\n",
        "        # Generate images\n",
        "        with torch.no_grad():\n",
        "            generated_images = self.diffusion.reverse_diffusion(x, self.model)\n",
        "\n",
        "        # Save generated images\n",
        "        fig, axs = plt.subplots(4, 4, figsize=(12, 12))\n",
        "        for i in range(num_samples):\n",
        "            img = generated_images[i].cpu()\n",
        "            img = (img + 1) / 2  # Denormalize\n",
        "            img = img.permute(1, 2, 0).numpy()\n",
        "            axs[i//4, i%4].imshow(img)\n",
        "            axs[i//4, i%4].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(samples_dir, f'samples_epoch_{epoch}.png'))\n",
        "        plt.close()\n",
        "\n",
        "def main():\n",
        "    # Configuration\n",
        "    data_dir = \"/root/.cache/kagglehub/datasets/pavansanagapati/images-dataset/versions/1\"\n",
        "    image_size = 64\n",
        "    batch_size = 32\n",
        "    num_epochs = 100\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = DiffusionTrainer(\n",
        "        data_dir=data_dir,\n",
        "        image_size=image_size,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    # Start training\n",
        "    trainer.train(num_epochs=num_epochs)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "jpifgR0uBHuo",
        "outputId": "b43be1ac-5416-47a7-96bb-f2ccb37edf76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/root/.cache/kagglehub/datasets/pavansanagapati/images-dataset/versions/1'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2b6de97a3720>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-2b6de97a3720>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;31m# Initialize trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m     trainer = DiffusionTrainer(\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-2b6de97a3720>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_dir, image_size, batch_size)\u001b[0m\n\u001b[1;32m    174\u001b[0m         ])\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         self.dataset = datasets.ImageFolder(\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> None:\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         samples = self.make_dataset(\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/root/.cache/kagglehub/datasets/pavansanagapati/images-dataset/versions/1'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qfBOS2uaUBBS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}