{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM9ZMmrwqf9xi1I3+dckO8U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Avniiii2606/Image-To-Image-Diffusion/blob/main/image_to_image_diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYGeHVjTFZDq",
        "outputId": "11529736-28da-4e58-a20a-6e6743ec633a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.7).\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/ashwingupta3012/human-faces/versions/1\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.7).\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/elibooklover/victorian400/versions/5\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "\n",
        "path = kagglehub.dataset_download(\"ashwingupta3012/human-faces\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "path1 = kagglehub.dataset_download(\"elibooklover/victorian400\")\n",
        "\n",
        "print(\"Path to dataset files:\", path1)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "import torch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " CUDA_LAUNCH_BLOCKING=1"
      ],
      "metadata": {
        "id": "kvQkCce3XGHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.distributed as dist\n",
        "import torch.nn.parallel\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "def setup_ddp(rank, world_size):\n",
        "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
        "    torch.cuda.set_device(rank)\n"
      ],
      "metadata": {
        "id": "hR1VZ5Lshhqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch torchvision torchaudio\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "id": "p_9pjkV3Xl3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "181a39b0-43b9-44bd-aa93-0d67452ea805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.6.0+cu118\n",
            "Uninstalling torch-2.6.0+cu118:\n",
            "  Would remove:\n",
            "    /usr/local/bin/torchfrtrace\n",
            "    /usr/local/bin/torchrun\n",
            "    /usr/local/lib/python3.11/dist-packages/functorch/*\n",
            "    /usr/local/lib/python3.11/dist-packages/torch-2.6.0+cu118.dist-info/*\n",
            "    /usr/local/lib/python3.11/dist-packages/torch/*\n",
            "    /usr/local/lib/python3.11/dist-packages/torchgen/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled torch-2.6.0+cu118\n",
            "Found existing installation: torchvision 0.21.0+cu118\n",
            "Uninstalling torchvision-0.21.0+cu118:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.11/dist-packages/torchvision-0.21.0+cu118.dist-info/*\n",
            "    /usr/local/lib/python3.11/dist-packages/torchvision.libs/libcudart.60cfec8e.so.11.0\n",
            "    /usr/local/lib/python3.11/dist-packages/torchvision.libs/libjpeg.1c1c4b09.so.8\n",
            "    /usr/local/lib/python3.11/dist-packages/torchvision.libs/libnvjpeg.70530407.so.11\n",
            "    /usr/local/lib/python3.11/dist-packages/torchvision.libs/libpng16.0364a1db.so.16\n",
            "    /usr/local/lib/python3.11/dist-packages/torchvision.libs/libsharpyuv.5c41a003.so.0\n",
            "    /usr/local/lib/python3.11/dist-packages/torchvision.libs/libwebp.54a0d02a.so.7\n",
            "    /usr/local/lib/python3.11/dist-packages/torchvision.libs/libz.d13a2644.so.1\n",
            "    /usr/local/lib/python3.11/dist-packages/torchvision/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled torchvision-0.21.0+cu118\n",
            "Found existing installation: torchaudio 2.6.0+cu118\n",
            "Uninstalling torchaudio-2.6.0+cu118:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.11/dist-packages/torchaudio-2.6.0+cu118.dist-info/*\n",
            "    /usr/local/lib/python3.11/dist-packages/torchaudio/*\n",
            "    /usr/local/lib/python3.11/dist-packages/torio/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled torchaudio-2.6.0+cu118\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (27 kB)\n",
            "Collecting torchvision\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torchvision-0.21.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torchaudio\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torchaudio-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.86)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Using cached https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl (848.7 MB)\n",
            "Using cached https://download.pytorch.org/whl/cu118/torchvision-0.21.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.5 MB)\n",
            "Using cached https://download.pytorch.org/whl/cu118/torchaudio-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.6.0+cu118 torchaudio-2.6.0+cu118 torchvision-0.21.0+cu118\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              },
              "id": "33a92371f399456a8ed8d0410b11d26e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import math\n",
        "\n",
        "class FaceTranslationDataset(Dataset):\n",
        "    \"\"\"Custom dataset for paired face images (painting/statue -> real)\"\"\"\n",
        "    def __init__(self, source_dir, target_dir, image_size=128):\n",
        "        # Use Path.rglob to search for images recursively in subfolders\n",
        "        self.source_paths = sorted(Path(source_dir).rglob('*.jpg'))\n",
        "        self.target_paths = sorted(Path(target_dir).rglob('*.jpg'))\n",
        "\n",
        "        # Print the number of images found\n",
        "        print(f\"Found {len(self.source_paths)} source images\")\n",
        "        print(f\"Found {len(self.target_paths)} target images\")\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.CenterCrop(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        # Ensure both source and target have the same number of images\n",
        "        return min(len(self.source_paths), len(self.target_paths))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        source_img = Image.open(self.source_paths[idx]).convert('RGB')\n",
        "        target_img = Image.open(self.target_paths[idx]).convert('RGB')\n",
        "\n",
        "        return {\n",
        "            'source': self.transform(source_img),\n",
        "            'target': self.transform(target_img)\n",
        "        }\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    \"\"\"Fixed cross attention module for conditioning\"\"\"\n",
        "    def __init__(self, channels, num_heads=8):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.channels = channels\n",
        "        self.scale = (channels // num_heads) ** -0.5\n",
        "\n",
        "        # Fix: Initialize GroupNorm with the correct number of channels\n",
        "        self.norm_x = nn.GroupNorm(1, channels)  # Use separate GroupNorms\n",
        "        self.norm_context = nn.GroupNorm(1, channels)  # For the context with 'channels' channels\n",
        "\n",
        "        self.to_q = nn.Conv2d(channels, channels, 1)\n",
        "        self.to_k = nn.Conv2d(channels, channels, 1)  # Change input channels to 'channels'\n",
        "        self.to_v = nn.Conv2d(channels, channels, 1)  # Change input channels to 'channels'\n",
        "        self.to_out = nn.Conv2d(channels, channels, 1)  # Initialize the context norm with the number of channels in the input tensor x\n",
        "\n",
        "        # Define context_proj here\n",
        "        self.context_proj = nn.Conv2d(in_channels=3, out_channels=channels, kernel_size=1) # Assuming context has 3 channels\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        batch, c, h, w = x.shape\n",
        "\n",
        "        # Project the context to the desired number of channels before normalization\n",
        "        context = self.context_proj(context)\n",
        "\n",
        "\n",
        "        # Normalize inputs using separate GroupNorms\n",
        "        x = self.norm_x(x)\n",
        "        context = self.norm_context(context)\n",
        "\n",
        "        # Create Q, K, V\n",
        "        q = self.to_q(x)\n",
        "        k = self.to_k(context)\n",
        "        v = self.to_v(context)\n",
        "\n",
        "        # Reshape for attention\n",
        "        q = q.view(batch, self.num_heads, c // self.num_heads, h * w)\n",
        "        k = k.view(batch, self.num_heads, c // self.num_heads, h * w)\n",
        "        v = v.view(batch, self.num_heads, c // self.num_heads, h * w)\n",
        "\n",
        "        # Transpose for attention computation\n",
        "        q, k, v = map(lambda t: t.transpose(-2, -1), (q, k, v))\n",
        "\n",
        "        # Attention\n",
        "        attention = torch.softmax(torch.matmul(q, k.transpose(-2, -1)) * self.scale, dim=-1)\n",
        "        out = torch.matmul(attention, v)\n",
        "\n",
        "        # Reshape back\n",
        "        out = out.transpose(-2, -1).reshape(batch, c, h, w)\n",
        "        return self.to_out(out) + x\n",
        "\n",
        "class ConditionalUNet(nn.Module):\n",
        "    \"\"\"Updated U-Net with fixed cross-attention\"\"\"\n",
        "    def __init__(self, in_channels=3, out_channels=3, time_emb_dim=128):\n",
        "        super().__init__()\n",
        "\n",
        "        # Time embedding\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
        "            nn.Linear(time_emb_dim, time_emb_dim * 2),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(time_emb_dim * 2, time_emb_dim)\n",
        "        )\n",
        "\n",
        "        # Initial convolution\n",
        "        self.conv_in = nn.Conv2d(in_channels, 64, 3, padding=1)\n",
        "\n",
        "        # Encoder\n",
        "        self.down1 = nn.ModuleList([\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            CrossAttention(128),\n",
        "            nn.Conv2d(128, 128, 3, padding=1)\n",
        "        ])\n",
        "\n",
        "        self.down2 = nn.ModuleList([\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            CrossAttention(128),\n",
        "            nn.Conv2d(128, 128, 3, padding=1)\n",
        "        ])\n",
        "\n",
        "        self.down3 = nn.ModuleList([\n",
        "            nn.Conv2d(128, 512, 3, padding=1),\n",
        "            CrossAttention(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1)\n",
        "        ])\n",
        "\n",
        "        # Middle\n",
        "        self.mid = nn.ModuleList([\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            CrossAttention(512),\n",
        "            nn.Conv2d(512, 512, 3, padding=1)\n",
        "        ])\n",
        "\n",
        "        # Decoder\n",
        "        self.up1 = nn.ModuleList([\n",
        "            nn.ConvTranspose2d(1024, 128, 2, stride=2),\n",
        "            CrossAttention(128),\n",
        "            nn.Conv2d(128, 128, 3, padding=1)\n",
        "        ])\n",
        "\n",
        "        self.up2 = nn.ModuleList([\n",
        "            nn.ConvTranspose2d(512, 128, 2, stride=2),\n",
        "            CrossAttention(128),\n",
        "            nn.Conv2d(128, 128, 3, padding=1)\n",
        "        ])\n",
        "\n",
        "        self.up3 = nn.ModuleList([\n",
        "            nn.ConvTranspose2d(128, 64, 2, stride=2),\n",
        "            CrossAttention(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1)\n",
        "        ])\n",
        "\n",
        "        # Final convolution\n",
        "        self.conv_out = nn.Conv2d(64, out_channels, 1)\n",
        "\n",
        "        # Downsample and upsample operations\n",
        "        self.downs = nn.ModuleList([\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.MaxPool2d(2)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, t, context):\n",
        "        # Time embedding\n",
        "        t = self.time_mlp(t)[:, :, None, None]  # Add spatial dimensions\n",
        "\n",
        "        # Initial conv\n",
        "        x = self.conv_in(x)\n",
        "\n",
        "        # Cache residuals\n",
        "        residuals = []\n",
        "\n",
        "        # Encoder\n",
        "        for i, down in enumerate([self.down1, self.down2, self.down3]):\n",
        "            residuals.append(x)\n",
        "            x = F.gelu(down[0](x))\n",
        "            x = down[1](x, context)  # Cross attention\n",
        "            x = F.gelu(down[2](x))\n",
        "            x = self.downs[i](x)\n",
        "\n",
        "        # Middle\n",
        "        x = F.gelu(self.mid[0](x))\n",
        "        x = self.mid[1](x, context)  # Cross attention\n",
        "        x = F.gelu(self.mid[2](x))\n",
        "\n",
        "        # Decoder\n",
        "        for i, up in enumerate([self.up1, self.up2, self.up3]):\n",
        "            x = up[0](torch.cat([x, residuals[-i-1]], dim=1))\n",
        "            x = up[1](x, context)  # Cross attention\n",
        "            x = F.gelu(up[2](x))\n",
        "\n",
        "        return self.conv_out(x)\n",
        "\n",
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    \"\"\"Time embedding module\"\"\"\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings\n",
        "\n",
        "def train(source_dir, target_dir, num_epochs=100, batch_size=1, device=\"cuda\", gradient_accumulation_steps=4): # add gradient_accumulation_steps\n",
        "    # Initialize dataset and dataloader\n",
        "    dataset = FaceTranslationDataset(source_dir, target_dir, image_size=128) # Reduce image size to 128x128\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "    # Initialize model and diffusion\n",
        "    model = ConditionalUNet().to(device)\n",
        "    diffusion = FaceTranslationDiffusion()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
        "            source_images = batch['source'].to(device)\n",
        "            target_images = batch['target'].to(device)\n",
        "\n",
        "            # Sample random timesteps\n",
        "            t = torch.randint(0, diffusion.noise_steps, (source_images.shape[0],)).to(device)\n",
        "\n",
        "            # Add noise to target images\n",
        "            noisy_target, noise = diffusion.noise_images(target_images, t)\n",
        "\n",
        "            # Predict noise\n",
        "            predicted_noise = model(noisy_target, t, source_images)\n",
        "\n",
        "             # Calculate loss\n",
        "            loss = F.mse_loss(noise, predicted_noise)\n",
        "\n",
        "            # Gradient Accumulation\n",
        "            loss = loss / gradient_accumulation_steps # scale loss\n",
        "            loss.backward()\n",
        "\n",
        "            if (batch_idx + 1) % gradient_accumulation_steps == 0: # update every n steps\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            # Optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Print progress\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Save sample generations periodically\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            save_sample_generations(model, diffusion, dataloader, epoch, device)\n",
        "\n",
        "def save_sample_generations(model, diffusion, dataloader, epoch, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        sample_source = next(iter(dataloader))['source'][:1].to(device)\n",
        "        sample_result = diffusion.sample(model, sample_source)\n",
        "\n",
        "        # Save the result\n",
        "        save_image(\n",
        "            torch.cat([sample_source, sample_result], dim=0),\n",
        "            f\"samples/epoch_{epoch+1}.png\",\n",
        "            normalize=True,\n",
        "            nrow=2\n",
        "        )\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation, PillowWriter\n",
        "import imageio\n",
        "from IPython.display import display, HTML\n",
        "import numpy as np\n",
        "from torchvision.utils import make_grid\n",
        "import io\n",
        "\n",
        "class FaceTranslationVisualizer:\n",
        "    def __init__(self, model, diffusion, device=\"cuda\"):\n",
        "        self.model = model\n",
        "        self.diffusion = diffusion\n",
        "        self.device = device\n",
        "\n",
        "    def tensor_to_image(self, tensor):\n",
        "        \"\"\"Convert tensor to numpy image\"\"\"\n",
        "        img = tensor.cpu().detach()\n",
        "        img = (img.clamp(-1, 1) + 1) / 2\n",
        "        img = img.permute(1, 2, 0).numpy()\n",
        "        return (img * 255).astype(np.uint8)\n",
        "\n",
        "    def create_comparison_grid(self, source_img, generated_img):\n",
        "        \"\"\"Create a grid with source and generated images\"\"\"\n",
        "        # Ensure inputs are in the right format\n",
        "        if isinstance(source_img, np.ndarray):\n",
        "            source_img = torch.from_numpy(source_img).permute(2, 0, 1)\n",
        "        if isinstance(generated_img, np.ndarray):\n",
        "            generated_img = torch.from_numpy(generated_img).permute(2, 0, 1)\n",
        "\n",
        "        # Create grid\n",
        "        grid = make_grid(\n",
        "            [source_img, generated_img],\n",
        "            nrow=2,\n",
        "            normalize=True,\n",
        "            value_range=(-1, 1)\n",
        "        )\n",
        "        return grid\n",
        "\n",
        "    def save_generation_process(self, source_image, save_path=\"generation_process.gif\",\n",
        "                              num_frames=50, fps=10):\n",
        "        \"\"\"Generate and save the transformation process as a GIF\"\"\"\n",
        "        self.model.eval()\n",
        "        frames = []\n",
        "\n",
        "        # Prepare source image\n",
        "        if not isinstance(source_image, torch.Tensor):\n",
        "            transform = torchvision.transforms.ToTensor()\n",
        "            source_image = transform(source_image).unsqueeze(0).to(self.device)\n",
        "            source_image = source_image * 2 - 1  # Scale to [-1, 1]\n",
        "\n",
        "        # Initialize with random noise\n",
        "        x = torch.randn((1, 3, self.diffusion.img_size, self.diffusion.img_size)).to(self.device)\n",
        "\n",
        "        step_size = self.diffusion.noise_steps // num_frames\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in reversed(range(0, self.diffusion.noise_steps, step_size)):\n",
        "                # Get current timestep\n",
        "                t = torch.tensor([i]).to(self.device)\n",
        "\n",
        "                # Predict noise and denoise\n",
        "                predicted_noise = self.model(x, t, source_image)\n",
        "\n",
        "                alpha = self.diffusion.alphas[i]\n",
        "                alpha_hat = self.diffusion.alpha_hat[i]\n",
        "                beta = self.diffusion.betas[i]\n",
        "\n",
        "                if i > 0:\n",
        "                    noise = torch.randn_like(x)\n",
        "                else:\n",
        "                    noise = torch.zeros_like(x)\n",
        "\n",
        "                x = 1 / torch.sqrt(alpha) * (\n",
        "                    x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise\n",
        "                ) + torch.sqrt(beta) * noise\n",
        "\n",
        "                # Create comparison frame\n",
        "                if i % step_size == 0:\n",
        "                    current_img = (x.clamp(-1, 1) + 1) / 2\n",
        "                    grid = self.create_comparison_grid(\n",
        "                        source_image[0],\n",
        "                        current_img[0]\n",
        "                    )\n",
        "                    grid_img = (grid.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
        "                    frames.append(Image.fromarray(grid_img))\n",
        "\n",
        "        # Save as GIF\n",
        "        frames[0].save(\n",
        "            save_path,\n",
        "            save_all=True,\n",
        "            append_images=frames[1:],\n",
        "            duration=1000/fps,\n",
        "            loop=0\n",
        "        )\n",
        "        return frames\n",
        "\n",
        "    def display_interactive(self, source_image, figsize=(12, 6)):\n",
        "        \"\"\"Display interactive visualization in notebook\"\"\"\n",
        "        frames = self.save_generation_process(source_image)\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=figsize)\n",
        "        plt.axis('off')\n",
        "\n",
        "        def animate(frame):\n",
        "            ax.clear()\n",
        "            ax.imshow(frame)\n",
        "            ax.axis('off')\n",
        "            ax.set_title('Left: Source Image | Right: Generated Image')\n",
        "\n",
        "        anim = FuncAnimation(\n",
        "            fig,\n",
        "            animate,\n",
        "            frames=frames,\n",
        "            interval=100,\n",
        "            repeat=True\n",
        "        )\n",
        "        plt.close()\n",
        "        return anim\n",
        "\n",
        "    def display_static_comparison(self, source_image, generated_image, figsize=(12, 6)):\n",
        "        \"\"\"Display static comparison of source and generated images\"\"\"\n",
        "        plt.figure(figsize=figsize)\n",
        "\n",
        "        grid = self.create_comparison_grid(source_image, generated_image)\n",
        "        grid_img = grid.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "        plt.imshow(grid_img)\n",
        "        plt.axis('off')\n",
        "        plt.title('Left: Source Image | Right: Generated Image')\n",
        "        plt.show()\n",
        "\n",
        "# Modify the training loop to include visualization\n",
        "def train_with_visualization_ddp(rank, world_size, source_dir, target_dir, num_epochs=100, batch_size=1, save_dir=\"results\"):\n",
        "    setup_ddp(rank, world_size)\n",
        "\n",
        "    device = torch.device(f\"cuda:{rank}\")\n",
        "    model = ConditionalUNet().to(device)\n",
        "    diffusion = FaceTranslationDiffusion()\n",
        "\n",
        "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank])\n",
        "\n",
        "    # Use DistributedSampler to distribute data across GPUs\n",
        "    dataset = FaceTranslationDataset(source_dir, target_dir, image_size=128)\n",
        "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler, num_workers=4)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        sampler.set_epoch(epoch)  # Ensure proper shuffling across epochs\n",
        "        model.train()\n",
        "\n",
        "        for batch in dataloader:\n",
        "            source_images = batch['source'].to(device)\n",
        "            target_images = batch['target'].to(device)\n",
        "\n",
        "            t = torch.randint(0, diffusion.noise_steps, (source_images.shape[0],)).to(device)\n",
        "            noisy_target, noise = diffusion.noise_images(target_images, t)\n",
        "            predicted_noise = model(noisy_target, t, source_images)\n",
        "\n",
        "            loss = F.mse_loss(noise, predicted_noise)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Save model only from rank 0 to avoid multiple saves\n",
        "        if rank == 0 and (epoch + 1) % 10 == 0:\n",
        "            torch.save(model.state_dict(), f\"{save_dir}/model_epoch_{epoch+1}.pth\")\n",
        "\n",
        "    dist.destroy_process_group()  # Clean up distributed training\n",
        "\n",
        "\n",
        "def demo_visualization(source_image_path):\n",
        "    # Load and preprocess source image\n",
        "    source_image = Image.open(source_image_path).convert('RGB')\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(128),\n",
        "        transforms.CenterCrop(128),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Add this line for normalization\n",
        "    ])\n",
        "    source_tensor = transform(source_image).unsqueeze(0)\n",
        "\n",
        "    # Initialize models\n",
        "    model = ConditionalUNet().to(\"cuda\")\n",
        "    diffusion = FaceTranslationDiffusion()\n",
        "    visualizer = FaceTranslationVisualizer(model, diffusion)\n",
        "\n",
        "    # Load model weights (assuming you have trained the model)\n",
        "    model.load_state_dict(torch.load(\"model_weights.pth\"))\n",
        "\n",
        "    # Generate and display transformation process\n",
        "    print(\"Generating transformation process...\")\n",
        "    anim = visualizer.display_interactive(source_tensor)\n",
        "\n",
        "    # Generate final image and display comparison\n",
        "    print(\"Generating final comparison...\")\n",
        "    with torch.no_grad():\n",
        "        generated_image = diffusion.sample(model, source_tensor.to(\"cuda\"))\n",
        "        visualizer.display_static_comparison(source_tensor[0], generated_image[0])\n",
        "\n",
        "    return anim\n",
        "\n",
        "\n",
        "import torch\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FaceTranslationDiffusion:\n",
        "    def __init__(self, noise_steps=1000, beta_start=1e-4, beta_end=0.02, img_size=128, device=\"cuda\"):\n",
        "        self.noise_steps = noise_steps\n",
        "        self.beta_start = beta_start\n",
        "        self.beta_end = beta_end\n",
        "        self.img_size = img_size\n",
        "        self.device = device\n",
        "\n",
        "        # Define beta schedule\n",
        "        self.betas = torch.linspace(beta_start, beta_end, noise_steps, device=self.device)  # betas on device\n",
        "        self.alphas = 1. - self.betas\n",
        "        self.alpha_hat = torch.cumprod(self.alphas, dim=0)  # alpha_hat on device\n",
        "\n",
        "    def noise_images(self, x, t):\n",
        "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n",
        "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]\n",
        "        ε = torch.randn_like(x)\n",
        "        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * ε, ε\n",
        "\n",
        "    def sample_timesteps(self, n):\n",
        "        return torch.randint(0, self.noise_steps, (n,), device=self.device)\n",
        "\n",
        "    def sample(self, model, source_image, n=1):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            x = torch.randn((n, 3, self.img_size, self.img_size), device=self.device)  # x on device\n",
        "            for i in tqdm(reversed(range(0, self.noise_steps)), desc='sampling loop time step', total=self.noise_steps):\n",
        "                t = torch.full((n,), i, device=self.device, dtype=torch.long)  # t on device\n",
        "                predicted_noise = model(x, t, source_image)\n",
        "                alpha = self.alphas[t][:, None, None, None]\n",
        "                alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
        "                beta = self.betas[t][:, None, None, None]\n",
        "                if i > 0:\n",
        "                    noise = torch.randn_like(x)\n",
        "                else:\n",
        "                    noise = torch.zeros_like(x)\n",
        "                x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
        "        # Return the generated image in the range [-1, 1] for consistency\n",
        "        return x.clamp(-1, 1)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage\n",
        "    source_dir = \"/root/.cache/kagglehub/datasets/elibooklover/victorian400/versions/5\"\n",
        "    target_dir = \"/root/.cache/kagglehub/datasets/ashwingupta3012/human-faces/versions/1\"\n",
        "\n",
        "    # Enable memory fragmentation avoidance\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "    train(\n",
        "        source_dir=source_dir,\n",
        "        target_dir=target_dir,\n",
        "        num_epochs=100,\n",
        "        batch_size=1,\n",
        "        gradient_accumulation_steps=4\n",
        "    )\n",
        "\n",
        "    source_image_path = \"/content/drive/MyDrive/victorian_potrait.jpg.png\"\n",
        "    animation = demo_visualization(source_image_path)\n",
        "\n",
        "    # Save animation\n",
        "    animation.save('transformation.gif', writer='pillow')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "eEiC92mDFmUu",
        "outputId": "b7dcfc6c-0f4d-4567-9e8c-4f87a1360a13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8 source images\n",
            "Found 6973 target images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 1/100:   0%|          | 0/8 [00:02<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.49 GiB is free. Process 33284 has 8.25 GiB memory in use. Of the allocated memory 8.12 GiB is allocated by PyTorch, and 2.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-120858f80b43>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"PYTORCH_CUDA_ALLOC_CONF\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"expandable_segments:True\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m     train(\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0msource_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0mtarget_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-120858f80b43>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(source_dir, target_dir, num_epochs, batch_size, device, gradient_accumulation_steps)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;31m# Predict noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0mpredicted_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoisy_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m              \u001b[0;31m# Calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-120858f80b43>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, t, context)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mresiduals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdown\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdown\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Cross attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdown\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdowns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-120858f80b43>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.49 GiB is free. Process 33284 has 8.25 GiB memory in use. Of the allocated memory 8.12 GiB is allocated by PyTorch, and 2.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    }
  ]
}